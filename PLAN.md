# Plan: Enterprise Kubernetes Migration & Refactorization (Final)

**TL;DR**: Transform 30+ Docker Compose services into production-ready Kubernetes infrastructure using kind clusters with self-hosted Zalando Postgres Operator (shared cluster, separate databases per tenant), Linkerd service mesh, automated tenant onboarding via ArgoCD ApplicationSets, full tenant labeling with NetworkPolicy isolation allowing all egress, all configuration in codebase. Clean slate approach enables cloud-native best practices from day one.

**Steps:**

1. **Create Kubernetes directory structure with kind configuration** - Establish `k8s/kind/cluster-config.yaml` (3 worker nodes, ingress-ready, local registry localhost:5001, extraPortMappings for 80/443/5432/3306/6379/9090/9097), `k8s/base/{databases,cache,apps,monitoring,networking,mcp}/`, `k8s/overlays/{dev,staging,prod}/`, `k8s/operators/{zalando-postgres,linkerd}/`, `k8s/charts/react-scuba/`, `k8s/policies/`, `k8s/tenants/` with `.tenant-template/` for automated onboarding

2. **Deploy Zalando Postgres Operator with shared cluster strategy** - Install Zalando Postgres Operator to `postgres-operator` namespace, create single PostgreSQL CR at `k8s/base/databases/postgres-shared-cluster.yaml` (3 replicas, 30GB per pod for all tenants, streaming replication, automatic failover, connection pooling via pgbouncer), configure per-tenant database creation via init SQL in `k8s/base/databases/tenant-db-init-configmap.yaml`, deploy shared MariaDB Galera StatefulSet at `k8s/base/databases/mariadb-shared-galera.yaml` (3 nodes, 15GB per pod), implement database isolation via PostgreSQL roles and MariaDB grants in `k8s/base/databases/tenant-grants-job.yaml`

3. **Setup kind cluster with local registry and automation** - Create `.devcontainer/kind/cluster-config.yaml` (containerdConfigPatches for localhost:5001 registry, node labels for storage classes, disable default CNI for Linkerd), add `.devcontainer/kind/setup-cluster.sh` (provision cluster → install local-path-provisioner → install Linkerd → install Zalando operator → install ArgoCD → install sealed-secrets → configure local registry), add `.devcontainer/kind/destroy-cluster.sh`, update [devcontainer.json](c:\react_scuba_runner\react-scuba\.devcontainer\devcontainer.json) features: kind, kubectl, helm 3.16, linkerd-cli, kustomize, kubeseal, argocd-cli, add postCreateCommand to run setup script

4. **Implement automated tenant onboarding with ApplicationSets** - Create `.tenant-template/` directory structure (kustomization.yaml, deployment.yaml, service.yaml, ingress.yaml, configmap.yaml, sealedsecret.yaml, networkpolicy.yaml) with `${TENANT_ID}` placeholders, deploy ArgoCD ApplicationSet at `k8s/argocd/applicationset-tenants.yaml` with git directory generator (scmProvider: github, path: `k8s/tenants/*/`, automated sync/prune/self-heal), add `k8s/tenants/README.md` with onboarding instructions (copy template → replace placeholders → git push → automatic deployment), configure tenant database creation Job triggered by ArgoCD sync hook

5. **Establish NetworkPolicies with tenant isolation and allow-all egress** - Create `k8s/policies/00-default-deny-ingress.yaml` (deny all ingress only), `k8s/policies/01-allow-all-egress.yaml` (allow all outbound for third-party APIs, DNS, external services), `k8s/policies/02-tenant-isolation.yaml` (allow ingress only from same-tenant pods via podSelector matchLabels tenant), `k8s/policies/03-db-access.yaml` (allow apps with tier=app → shared databases on 5432,3306 with tenant label matching), `k8s/policies/04-cache-access.yaml` (apps → redis:6379, memcached:11211), `k8s/policies/05-monitoring-scrape.yaml` (prometheus → all pods), `k8s/policies/06-ingress-allow.yaml` (nginx ingress → apps), `k8s/policies/07-mcp-discovery.yaml` (mcp tier → all for observability)

6. **Convert application Deployments with tenant labels and shared DB connection** - Transform [node-api](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\services\api\docker-compose.api.yml) to `k8s/base/apps/api-deployment.yaml` with `linkerd.io/inject: enabled`, tenant label `tenant: ${TENANT}`, environment variables for tenant-specific DB (DB_NAME=${TENANT}_scuba_db, DB_USER=${TENANT}_user), HorizontalPodAutoscaler (2-10 replicas, 70% CPU), add health endpoints to [server/apps/api/src/index.js](c:\react_scuba_runner\react-scuba\server\apps\api\src\index.js), update [server/apps/api/src/db/connection.js](c:\react_scuba_runner\react-scuba\server\apps\api\src\db\connection.js) to use tenant-specific database from env vars, convert [node-web](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\services\web\docker-compose.web.yml) to `k8s/base/apps/web-deployment.yaml`, implement graceful shutdown with 30s drain

7. **Externalize secrets to SealedSecrets with tenant separation** - Install sealed-secrets-controller to `kube-system`, create `k8s/sealed-secrets/shared-postgres.yaml` for main postgres admin credentials (encrypted from [devcontainer.env](c:\react_scuba_runner\react-scuba\devcontainer.env) line 7), add `k8s/sealed-secrets/shared-mariadb.yaml` (line 23), create tenant-specific sealed secrets template at `.tenant-template/sealedsecret-tenant-db.yaml` (tenant db user/password, GitHub token, API keys), generate tenant secrets via `kubeseal` in onboarding script, create shared ConfigMaps at `k8s/base/configmaps/` for app configs from [api/src/index.js](c:\react_scuba_runner\react-scuba\server\apps\api\src\index.js), override per-tenant in `k8s/tenants/{client}/configmap.yaml` from [server/clients/](c:\react_scuba_runner\react-scuba\server\clients) configs

8. **Deploy Linkerd service mesh with automated injection** - Install Linkerd control plane to `linkerd` namespace via `k8s/operators/linkerd/install.yaml` (identity trust anchor auto-generated, 90-day rotation), enable proxy injection on namespaces via label `linkerd.io/inject: enabled` in `k8s/base/namespaces.yaml`, create ServiceProfiles at `k8s/base/linkerd/service-profiles/` for all 30 services (10s timeout, 3 retries on 5xx, 5s backoff), configure HTTPRoutes with tenant header routing `X-Tenant-ID` via ingress annotation, deploy Linkerd Viz to `linkerd-viz` namespace for dashboard, add Linkerd Jaeger extension for distributed tracing

9. **Implement observability stack with tenant label filtering** - Install Prometheus Operator to `monitoring` namespace, convert [prometheus](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\monitoring\prometheus\docker-compose.prometheus.yml) to Prometheus CR at `k8s/base/monitoring/prometheus.yaml` (30-day retention, 50GB storage, relabelConfigs to preserve tenant labels), deploy Grafana Operator with Grafana CR at `k8s/base/monitoring/grafana.yaml` (pre-configured dashboards with tenant dropdown variable), create ServiceMonitors for all services at `k8s/base/monitoring/servicemonitors/`, install Jaeger Operator with Jaeger CR (tenant tag propagation), deploy Fluent Bit DaemonSet at `k8s/base/monitoring/fluentbit-daemonset.yaml` (tenant log separation via kubernetes.labels.tenant)

10. **Convert cache services to StatefulSets with tenant-aware configuration** - Transform [redis](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\cache\redis\docker-compose.redis.yml) to `k8s/base/cache/redis-statefulset.yaml` (single 3-replica cluster with Redis Sentinel, tenant key prefixing via app logic, anti-affinity rules, 5GB per pod), convert [memcached](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\cache\memcached\docker-compose.memcached.yml) to Deployment (stateless, 3 replicas, consistent hashing), transform [minio](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\cache\minio\docker-compose.minio.yml) to StatefulSet (4 nodes distributed mode, 10GB per pod, tenant buckets auto-created via init container, IAM policies per tenant)

11. **Establish nginx Ingress Controller with tenant routing** - Deploy nginx-ingress-controller to `ingress-nginx` namespace from `k8s/base/networking/nginx-ingress-deployment.yaml`, create base Ingress template at `.tenant-template/ingress.yaml` with host-based routing (`${TENANT}.localhost` for dev, `${TENANT}.example.com` for prod), add tenant header injection via nginx annotation `nginx.ingress.kubernetes.io/configuration-snippet`, configure TLS with cert-manager ClusterIssuer (Let's Encrypt staging for dev, prod for staging/prod), implement rate limiting per tenant (1000 req/min) via annotation, add request ID propagation for tracing

12. **Migrate MCP services with tenant-aware discovery** - Convert [python-mcp-sidecar](c:\react_scuba_runner\react-scuba\docker-compose.yml) to Deployment at `k8s/base/mcp/python-mcp-deployment.yaml` (2 replicas per tenant, tenant label for isolation, environment variable MCP_TENANT_ID), transform [mcp-discovery-agent](c:\react_scuba_runner\react-scuba\docker-compose.yml) to Deployment with ServiceAccount for cluster-wide discovery at `k8s/base/mcp/discovery-deployment.yaml`, update discovery logic to filter by tenant label, create Service for discovery API, add NetworkPolicy allowing mcp → all for metrics collection

13. **Optimize Dockerfiles for production Kubernetes** - Refactor [api/dockerfile](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\services\api\dockerfile): stage 1 (node:22-alpine with full devDeps), stage 2 (npm prune --production), stage 3 (distroless/nodejs22-debian12, copy only production node_modules and built code, 800MB → 120MB), update [web/dockerfile](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\services\web\dockerfile): stage 1 (node:22-alpine vite build), stage 2 (nginx:1.27-alpine with optimized config, copy dist/, 600MB → 50MB), optimize [python-mcp/dockerfile](c:\react_scuba_runner\react-scuba\.devcontainer\infrastructure\mcp-servers\python\dockerfile) (python:3.14t-slim, UV for deps, remove build tools from final, 450MB → 180MB), add comprehensive .dockerignore files, implement non-root user (UID 1000)

14. **Implement storage with local-path-provisioner and tenant PVCs** - Deploy Rancher local-path-provisioner to `kube-system` for dynamic PV provisioning on kind nodes, define 3 StorageClasses in `k8s/base/storage/storage-classes.yaml` (fast-ssd: local-path with waitForFirstConsumer, standard: local-path with immediate, backup: retain policy), convert shared service volumes to PVCs in `k8s/base/storage/` (postgres-shared: 90GB, mariadb-shared: 45GB, redis-data: 15GB, minio-data: 40GB, prometheus-data: 50GB, grafana-data: 10GB, ollama-models: 50GB), create per-tenant PVCs template at `.tenant-template/pvc.yaml` (api-logs: 5GB, web-cache: 2GB, mcp-state: 1GB), add VolumeSnapshots CronJob for databases (daily 2AM UTC), deploy Velero with restic for backup to minio bucket

15. **Harden security with Pod Security Standards and tenant RBAC** - Enable Pod Security Admission controller, apply restricted PSS to all namespaces in `k8s/base/namespaces.yaml` (securityContext: runAsNonRoot, readOnlyRootFilesystem, drop ALL capabilities), create shared ServiceAccount at `k8s/base/rbac/app-serviceaccount.yaml`, define per-tenant Roles in `.tenant-template/role.yaml` (get/list/watch own resources only), create RoleBindings, implement cluster-level Roles for operators in `k8s/base/rbac/cluster-roles.yaml`, deploy OPA Gatekeeper with constraints at `k8s/policies/gatekeeper/` (require tenant label, enforce resource limits min 100m/128Mi max 2000m/2Gi, deny privileged, require probes), add Trivy admission webhook for CVE scanning (reject HIGH/CRITICAL), enable audit logging with tenant field extraction

16. **Create Helm chart with automated tenant value generation** - Generate Helm chart at `k8s/charts/react-scuba/` (Chart.yaml with dependencies: postgresql 15.x, redis 19.x, ingress-nginx 4.x), create base values.yaml with global settings, add helper template `_helpers.tpl` for tenant label generation, parameterize all resources with `.Values.tenant` object, create script `k8s/scripts/generate-tenant-values.sh` to auto-generate values files from [server/clients/](c:\react_scuba_runner\react-scuba\server\clients) config.json (reads name, domain, theme, features), add hooks: pre-install (create database), post-install (run migrations), pre-delete (backup data), implement tests in `templates/tests/` for connectivity validation

17. **Setup ArgoCD for GitOps with automated tenant sync** - Deploy ArgoCD to `argocd` namespace with HA (3 replicas), create ApplicationSet at `k8s/argocd/applicationset-tenants.yaml` with git directory generator (repoURL: <https://github.com/DeanLuus22021994/react-scuba>, path: `k8s/tenants/*`, exclude `.tenant-template`, automated syncPolicy: prune true, selfHeal true, retry limit 5), configure ArgoCD to use Helm chart with client-specific values from `k8s/tenants/{client}/values.yaml`, enable webhook sync from GitHub (push to k8s/tenants triggers immediate sync), add ArgoCD notifications for Slack on sync status, configure SSO with GitHub OAuth, implement RBAC (developers: read-only, ops: full access)

18. **Establish CI/CD pipeline with GitHub Actions** - Create `.github/workflows/build-and-push.yml` (triggers on push to server/, builds all Dockerfiles with BuildKit, scans with Trivy, tags with git SHA + latest, pushes to kind registry localhost:5001 and GitHub Container Registry ghcr.io), add `.github/workflows/deploy-manifests.yml` (validates K8s manifests with kubeval, applies to kind cluster, waits for rollout, runs smoke tests), create `.github/workflows/tenant-onboarding.yml` (triggers on new directory in k8s/tenants/, generates values from server/clients config, runs helm lint, creates PR), add `.github/workflows/security-audit.yml` (weekly: kubesec scan, kube-bench CIS benchmarks, falco rules validation, dependency updates), configure branch protection (require CI pass)

19. **Configure Skaffold for automated local development** - Create `.devcontainer/skaffold.yaml` with profiles per tenant from `k8s/tenants/` directories (build: local docker with cache, tag: dev-${GIT_SHA}, deploy: helm upgrade with values-${TENANT}.yaml, portForward: api:8003→3000, web:5173→5173, grafana:3000→3000, postgres:5432→5432), add file watching with sync rules (server/apps/api/src → /app hot reload, server/apps/web/src → rebuild), configure resource requirements for local dev (limits: 500m CPU, 512Mi RAM per service), enable Linkerd injection in dev, integrate with [devcontainer.json](c:\react_scuba_runner\react-scuba\.devcontainer\devcontainer.json) postStartCommand: `skaffold dev --port-forward --cleanup=false`, add `skaffold debug` support for remote debugging with VS Code

20. **Create comprehensive validation and automation tooling** - Refactor [validate-toc.ps1](c:\react_scuba_runner\react-scuba\.devcontainer\validate-toc.ps1) to `validate-k8s.ps1` with functions: Test-KindCluster (verify nodes ready, check CNI), Test-K8sManifests (kubectl apply --dry-run=server --validate=true for all yamls), Test-TenantLabels (scan all resources, verify tenant label present), Test-NetworkPolicies (validate ingress/egress rules, check for gaps), Test-HelmCharts (helm lint, helm template, kubeval output), Test-Linkerd (linkerd check --pre, linkerd viz check), Test-DatabaseConnections (pg_isready for each tenant DB), add script `k8s/scripts/onboard-tenant.sh` (interactive prompts for client details, generates manifests from template, creates sealed secrets, commits to git, triggers ArgoCD sync), create `k8s/scripts/offboard-tenant.sh` (backup data, delete namespace, prune ArgoCD app), update tasks.json with K8s validation tasks, add pre-commit hooks for manifest validation

**Open Questions:**

1. **Database Connection Pooling Strategy?** Use pgbouncer sidecar per app pod (2x pods, transaction pooling) vs single pgbouncer deployment (simpler, session pooling, potential bottleneck at 1000+ connections)? Affects database connection limits and app reliability.

2. **Tenant Database Provisioning Timing?** Create all tenant databases at PostgreSQL cluster init (faster onboarding, wastes resources for unused tenants) vs create on-demand via ArgoCD hook (efficient, adds 30s to first deployment)? Impacts initial setup complexity vs runtime efficiency.

3. **Resource Quota Granularity?** Set ResourceQuotas per tenant namespace (strong limits, requires separate namespaces, 3x overhead) vs cluster-wide LimitRanges only (simpler, relies on fair scheduling, no hard tenant caps)? Affects multi-tenant isolation and resource guarantees.
