# ==============================================================================
# docker-compose.gpu.yml - GPU & AI Services
# ==============================================================================
# Purpose: NVIDIA GPU acceleration, LLM inference, AI workloads
# Services: NVIDIA Device Plugin, Ollama LLM inference
# Network: mcp-cluster (172.28.0.0/16)
# Usage: docker-compose -f docker-compose.yml -f .devcontainer/infrastructure/compose/gpu.yml up
# Requirements: NVIDIA Docker runtime, CUDA 12.x
# ==============================================================================

services:
  # ==== GPU Support ====
  nvidia-device-plugin:
    build:
      context: ../gpu
      dockerfile: dockerfile
    image: nvidia-device-plugin:latest
    pull_policy: build
    container_name: nvidia-device-plugin
    hostname: nvidia-device-plugin
    privileged: true
    networks:
      mcp-cluster:
        ipv4_address: 172.28.0.80
    ports:
      - "9400:9400"
    env_file:
      - ../../../devcontainer.env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/usr/local/bin/gpu-health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # ==== LLM Inference Service ====
  # GPU Access: Direct host GPU via Docker Compose device requests
  # VRAM Limit: Scales to available GPU memory
  # Supported Models:
  #   - smollm2:latest (1.7GB, 100% GPU)
  #   - codegeex4:9b-all-q3_K_M (5.1GB, 100% GPU)
  #   - mistral:latest (7.3GB, quantized)
  # Configuration: Optimized for single concurrent inference, no CPU offload
  ollama-llm:
    build:
      context: ../services/ollama
      dockerfile: dockerfile
    image: ollama-llm:latest
    pull_policy: build
    container_name: ollama-llm
    hostname: ollama-llm
    networks:
      mcp-cluster:
        ipv4_address: 172.28.0.59
    ports:
      - "11435:11434"
      - "9401:9401"
    env_file:
      - ../../../devcontainer.env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_GPU_OVERHEAD=0
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_QUEUE=1
      - OLLAMA_SCHED_SPREAD=false
      - OLLAMA_CONTEXT_LENGTH=2048
      - OLLAMA_NUM_GPU=999
      - OLLAMA_GPU_LAYERS=999
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_DEBUG=0
    volumes:
      - react_scuba_ollama-models:/root/.ollama/models
      - C:/shared_memory/ollama-cache:/host-cache:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "/usr/local/bin/ollama-health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    depends_on:
      nvidia-device-plugin:
        condition: service_started
        required: false



# Volumes are defined in main docker-compose.yml to avoid conflicts
